{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Feature Engineering Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Engineer features for Classification, Regression and Cluster models\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/cleaned/TrainSet.csv\n",
        "* inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* generate a list with variables to engineer\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        "\n",
        "\n",
        "* Feature Engineering Transformers\n",
        "  * Ordinal categorical encoding: `['gender', 'Partner', Dependents', 'PhoneService','MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup','DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies','Contract', 'PaperlessBilling', 'PaymentMethod']`\n",
        "  * Smart Correlation Selection: `['OnlineSecurity', 'DeviceProtection', 'TechSupport']`\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGT0ZCtwFAFv"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcidnQspZztu"
      },
      "source": [
        "! pip install pandas-profiling==2.11.0\n",
        "! pip install feature-engine==1.0.2\n",
        "! pip install ppscore==1.2.0\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n",
        "  * Typically the output will be /device:GPU:0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicMedgXzMgS"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Uczzm_zXI4"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1q2QBwkcIH2"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXtmJPYKzasz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Token: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMP7Pjvwpm2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPGQ3xa0dH1"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4V8x_AF1Euv"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSC85enBhQ4N"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UTydg5Xwqiu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-5uhLCk0lUJ"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3ns1Tl0_MS"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yrqRnwhhXQY"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "import uuid\n",
        "file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "print(\"\\n\\n\")\n",
        "os.remove(f\"{file_name}.txt\")\n",
        "! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdAGw4Zwssu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6h-QeuCa2_U"
      },
      "source": [
        "Train Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ELZj83tF1g"
      },
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNJOqym1a38e"
      },
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-WRjItbiOs6"
      },
      "source": [
        "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      },
      "source": [
        "# Pandas Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edx7E2Yr4sZ5"
      },
      "source": [
        "Quick exploration "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyi3gi2-_q1j"
      },
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvwabO0JsmYW"
      },
      "source": [
        "# Correlation and PPS Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qufL2HWrF8ig"
      },
      "source": [
        "Supporting functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Zy_MglsmYo"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "def heatmap_corr(df,threshold):\n",
        "  if len(df.columns) > 1:\n",
        "    mask = np.zeros_like(df, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    mask[abs(df) < threshold] = True\n",
        "\n",
        "    fig, axes = plt.subplots(figsize=(20,12))\n",
        "    sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                     mask=mask, cmap='viridis', annot_kws={\"size\": 8}, ax=axes\n",
        "                     )\n",
        "    axes.set_yticklabels(df.columns, rotation = 0)\n",
        "    plt.ylim(len(df.columns),0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df,threshold):\n",
        "    if len(df.columns) > 1:\n",
        "\n",
        "      mask = np.zeros_like(df, dtype=np.bool)\n",
        "      mask[abs(df) < threshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,12))\n",
        "      ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                        mask=mask,cmap='rocket_r', annot_kws={\"size\": 8})\n",
        "      \n",
        "      plt.ylim(len(df.columns),0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "  df_corr_spearman = df.corr(method=\"spearman\")\n",
        "  df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "  pps_matrix_raw = pps.matrix(df)\n",
        "  pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "  pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "  print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "  print(pps_score_stats.round(3))\n",
        "\n",
        "  return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,CorrThreshold,PPS_Threshold):\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"* Analyze how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "  print(\"* Analyze multi colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "  print(\"It evaluates monotonic relationship \\n\")\n",
        "  heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "  print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "  heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "  print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "  heatmap_pps(df=pps_matrix,threshold=PPS_Threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryNo1VnXSK9K"
      },
      "source": [
        "Calculate Correlations and Power Predictive Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Z4SXf6GbED"
      },
      "source": [
        "dataset = pd.concat([TrainSet, TestSet], axis=0)\n",
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ-0L4PiSPEK"
      },
      "source": [
        "Display at Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioE3yuC4Q7QK"
      },
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,\n",
        "                  CorrThreshold=0.6, PPS_Threshold=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZsS4AdFjUYH"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqEHqNwyrucq"
      },
      "source": [
        "## Custom function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuEsBAljDsLO"
      },
      "source": [
        "Custom functions for engineering variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "masShIoqzf2b"
      },
      "source": [
        "from feature_engine import transformation as vt\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser, EqualWidthDiscretiser\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import RareLabelEncoder\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "\n",
        "\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set(style=\"darkgrid\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineering(df,analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape \n",
        "    - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ### Check analyis type\n",
        "    allowed_types= f\"'numerical', 'outlier', 'discretization', 'ordinal_encoder' or 'outlier_winsorizer'. \"\n",
        "    if analysis_type == None:\n",
        "      raise SystemExit(f\"You should pass analysis_type parameter as one of the following options: {allowed_types} \")\n",
        "    if analysis_type not in allowed_types:\n",
        "      raise SystemExit(f\"analysis_type argument should be either {allowed_types}\")\n",
        "\n",
        "\n",
        "    ### Set suffix colummns acording to analysis_type\n",
        "    if analysis_type=='numerical':\n",
        "      list_column_transformers = [\"lte\",\"lt10\",\"rt\", \"pt\",\"bct\",\"yj\"]\n",
        "    \n",
        "    elif analysis_type=='outlier':\n",
        "      list_column_transformers = [\"lt\",\"rt\", \"pt\",\"bct\",\"yj\"]\n",
        "    \n",
        "    elif analysis_type=='discretization':\n",
        "      list_column_transformers = ['equal_frequency_5intervals',\n",
        "                                  'equal_frequency_10intervals',\n",
        "                                  'equal_width_5intervals',\n",
        "                                  'equal_width_10intervals']\n",
        "    \n",
        "    elif analysis_type=='ordinal_encoder':\n",
        "      list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type=='outlier_winsorizer':\n",
        "      list_column_transformers = ['gaussian', 'iqr']\n",
        "\n",
        "\n",
        "\n",
        "    # empty engineered dataframe\n",
        "    df_feat_eng = pd.DataFrame([]) \n",
        "    \n",
        "    for column in df.columns:\n",
        "\n",
        "      ### create additional columns (column_method) to apply the methods\n",
        "      df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "      for method in list_column_transformers:\n",
        "        df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "        \n",
        "      ### Apply transformers in respectives column_transformers\n",
        "      if analysis_type=='numerical':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_Numerical(df_feat_eng,column)\n",
        "      \n",
        "      elif analysis_type=='discretization':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_Discretization(df_feat_eng,column)\n",
        "      \n",
        "      elif analysis_type=='outlier_winsorizer':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_OutlierWinsorizer(df_feat_eng,column)\n",
        "\n",
        "      elif analysis_type=='ordinal_encoder':\n",
        "        df_feat_eng,list_applied_transformers = FeatEngineering_CategoricalEncoder(df_feat_eng,column)\n",
        "\n",
        "      \n",
        "      # For each variable, assess how the transformations perform\n",
        "      print(f\"* Variable Analyzed: {column}\")\n",
        "      print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "      for col in [column] + list_applied_transformers:\n",
        "        \n",
        "        if analysis_type!='ordinal_encoder':\n",
        "          DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "        \n",
        "        else:\n",
        "          if col == column: \n",
        "            DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "          else:\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "  plt.figure(figsize=(20, 5))\n",
        "  sns.countplot(data=df_feat_eng, x=col,palette=['#432371'],order = df_feat_eng[col].value_counts().index)\n",
        "  plt.xticks(rotation=90) \n",
        "  plt.suptitle(f\"{col}\", fontsize=30,y=1.05)        \n",
        "  plt.show();\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "\n",
        "    fig, (ax1, ax2,ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    sns.histplot(data=df, x=variable, kde=True,element=\"step\",ax=ax1) \n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=ax2)\n",
        "    sns.boxplot(x=df[variable],ax=ax3)\n",
        "    \n",
        "    # analysis on outliers\n",
        "    # shapiro analysis\n",
        "    ax1.set_title('Histogram')\n",
        "    ax2.set_title('Probability Plot')\n",
        "    ax3.set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30,y=1.05)\n",
        "    plt.show();\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng,column):\n",
        "  list_methods_worked = []\n",
        "  try:\n",
        "    encoder= OrdinalEncoder(encoding_method='arbitrary', variables = [f\"{column}_ordinal_encoder\"])\n",
        "    df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_ordinal_encoder\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### Winsorizer gaussian\n",
        "  try: \n",
        "    disc=Winsorizer(\n",
        "        capping_method='gaussian', tail='both', fold=3,variables = [f\"{column}_gaussian\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_gaussian\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_gaussian\"],axis=1,inplace=True)\n",
        "\n",
        "  ### Winsorizer iqr\n",
        "  try: \n",
        "    disc=Winsorizer(\n",
        "        capping_method='iqr', tail='both', fold=3,variables = [f\"{column}_iqr\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_iqr\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_iqr\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def FeatEngineering_Discretization(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### EqualFrequencyDiscretiser\n",
        "  try: \n",
        "    disc= EqualFrequencyDiscretiser(q=5,variables = [f\"{column}_equal_frequency_5intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_frequency_5intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_frequency_5intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### EqualFrequencyDiscretiser\n",
        "  try: \n",
        "    disc= EqualFrequencyDiscretiser(q=10,variables = [f\"{column}_equal_frequency_10intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_frequency_10intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_frequency_10intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### EqualWidthDiscretiser\n",
        "  try: \n",
        "    disc= EqualWidthDiscretiser(bins=5,variables = [f\"{column}_equal_width_5intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_width_5intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_width_5intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### EqualWidthDiscretiser\n",
        "  try: \n",
        "    disc= EqualWidthDiscretiser(bins=10,variables = [f\"{column}_equal_width_10intervals\"])\n",
        "    df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_equal_width_10intervals\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_equal_width_10intervals\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "  \n",
        "  \n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng,column):\n",
        "\n",
        "  list_methods_worked = []\n",
        "\n",
        "  ### LogTransformer base e\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_lte\"])\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_lte\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_lte\"],axis=1,inplace=True)\n",
        "\n",
        "    ### LogTransformer base 10\n",
        "  try: \n",
        "    lt = vt.LogTransformer(variables = [f\"{column}_lt10\"],base='10')\n",
        "    df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_lt10\")\n",
        "  except: \n",
        "    df_feat_eng.drop([f\"{column}_lt10\"],axis=1,inplace=True)\n",
        "\n",
        "  ### ReciprocalTransformer\n",
        "  try:\n",
        "    rt = vt.ReciprocalTransformer(variables = [f\"{column}_rt\"])\n",
        "    df_feat_eng =  rt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_rt\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_rt\"],axis=1,inplace=True)\n",
        "\n",
        "  ### PowerTransformer\n",
        "  try:\n",
        "    pt = vt.PowerTransformer(variables = [f\"{column}_pt\"])\n",
        "    df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_pt\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_pt\"],axis=1,inplace=True)\n",
        "\n",
        "  ### BoxCoxTransformer\n",
        "  try:\n",
        "    bct = vt.BoxCoxTransformer(variables = [f\"{column}_bct\"])\n",
        "    df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_bct\")\n",
        "  except:\n",
        "    df_feat_eng.drop([f\"{column}_bct\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  ### YeoJohnsonTransformer\n",
        "  try:\n",
        "    yjt = vt.YeoJohnsonTransformer(variables = [f\"{column}_yj\"])\n",
        "    df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "    list_methods_worked.append(f\"{column}_yj\")\n",
        "  except:\n",
        "        df_feat_eng.drop([f\"{column}_yj\"],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  return df_feat_eng,list_methods_worked\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yh2FMDd4wWZ"
      },
      "source": [
        "## Feature Engineering Spreadsheet Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUsyE39kjWrI"
      },
      "source": [
        "At this stage, there are no missing data in your Train and Test sets.\n",
        "\n",
        "\n",
        "Now you are looking to engineer (**to transform**), your variables, so the Machine Learning model will better learn the relationships among the variables (features and labels).\n",
        "  * It is important to run a quick EDA to asess variables distribution shape. Machine Learning models learn better when the distribution is normal. To engineer that, you can use transformers in packages like feature-engine or sklearn.\n",
        "  * You can also use your business acumen and technical expertise to create new variables. For example, imagine if your dataset is about your orange juice company operation. There is a variable called \"revenue\" and other called \"volume\", you divide revenue by volume to know how much money you make per liter of manufactured juice\n",
        "\n",
        "---   \n",
        "\n",
        "**REMINDER**\n",
        "  * The transformers decided in this notebook will serve as base to construct the **ML Pipeline** for the upcoming notebooks!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvG1I9mHmNWC"
      },
      "source": [
        "**Strategy**\n",
        "\n",
        "\n",
        "* **1 - Do a quick EDA and evaluate distribution from all variables. List all variables and potential transformation to apply on each (including no transformation)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWsUzp8IoVxw"
      },
      "source": [
        "\n",
        "* **2 - Consider the following template to help your engineering process**\n",
        "\n",
        "  * 1 - Select variable(s)\n",
        "  * 2 - Select the transformer(s)\n",
        "  * 3 - Create a separate dataframe, for that variable(s)\n",
        "  * 4 - Create engineered variables(s) applying the transformation(s)\n",
        "  * 5 - Assess engineered variables distribution and select most suitable transformation\n",
        "  * 6 - If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-9iRH_ZEMoJ"
      },
      "source": [
        "## Dealing with Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GT8e9W_onJj"
      },
      "source": [
        "### Categorical Enconding - Ordinal: replaces categories by ordinal numbers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwG_KMpOonJu"
      },
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ii9nXtJonJv"
      },
      "source": [
        "variables_engineering= ['gender', 'Partner', 'Dependents', 'PhoneService',\n",
        "                        'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
        "                        'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
        "                        'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
        "\n",
        "variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzOOSSzsonJw"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvQf2rkoonJx"
      },
      "source": [
        "from feature_engine.encoding import OrdinalEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t32melBMonJy"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJJBWQlconJy"
      },
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mk058LZonJz"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method for each variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKEuWXrtonJz"
      },
      "source": [
        "df_engineering = FeatureEngineering(df=df_engineering,analysis_type='ordinal_encoder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfAiPd8DonJz"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effective\n",
        "  * Location': ordinal encoder - boxplot looks good, qq plot could be better - evaluate additional need for numerical transformation\n",
        "  * 'WindGustDir: ordinal encoder - boxplot looks good, qq plot could be better - evaluate additional need for numerical transformation\n",
        "  * 'WindDir9am': ordinal encoder - boxplot looks bit skewed, qq plot could be better - evaluate additional need for numerical transformation\n",
        "  * 'WindDir3pm': ordinal encoder - boxplot looks good, qq plot could be better - evaluate additional need for numerical transformation\n",
        "  * 'State': ordinal encoder - transformed to numerical - evaluate additional need for numerical transformation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM2xZ-YTonJ0"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV4pTKpHonJ0"
      },
      "source": [
        "# the steps are: \n",
        "# 1 - select given transformation and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "variable_categ_enc = variables_engineering\n",
        "if variable_categ_enc: \n",
        "  encoder = OrdinalEncoder(encoding_method='arbitrary', variables = variable_categ_enc)\n",
        "  TrainSet = encoder.fit_transform(TrainSet)\n",
        "  TestSet = encoder.transform(TestSet)\n",
        "\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkyO7KsuyG5A"
      },
      "source": [
        "### Handle Outliers (Winsorizer:caps maximum and/or minimum values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l4eVnHNyG5E"
      },
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB9PqsbGyffD"
      },
      "source": [
        "* **Quick reminder: The variable(s) has(ve) to numerical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-dEF_P4yG5E"
      },
      "source": [
        "variables_engineering = ['tenure','MonthlyCharges','TotalCharges']\n",
        "variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1-fcQnUyG5H"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOkGuVWayG5I"
      },
      "source": [
        "from feature_engine.outliers import Winsorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUA_uA1_yG5I"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrplTb39yG5I"
      },
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQqqdvd3yG5J"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZXMUZgEyG5J"
      },
      "source": [
        "df_engineering = FeatureEngineering(df=df_engineering,analysis_type='outlier_winsorizer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5TyFpY1yG5J"
      },
      "source": [
        "* For each variable, write you conclusion on how the transformation(s) look(s) to be effective\n",
        "  * For all variables - it didn't improved the boxplot distribution or qq plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpgHsRqyG5K"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformation to the Train and Test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q6_oTrsyG5K"
      },
      "source": [
        "# the steps are: \n",
        "# 1 - select given transformation and respective variable(s)\n",
        "# 2 - create transformer\n",
        "# 3 - fit_transform into TrainSet\n",
        "# 4 - transform into TestSet\n",
        "\n",
        "\n",
        "### Winsorizer Gaussian\n",
        "variable_out_gaussian = []\n",
        "if variable_out_gaussian:\n",
        "  out_transf =Winsorizer(capping_method='gaussian',tail='both', fold=3,variables = variable_out_gaussian)\n",
        "  TrainSet = out_transf.fit_transform(TrainSet)\n",
        "  TestSet = out_transf.transform(TestSet)\n",
        "\n",
        "### Winsorizer IQR\n",
        "variable_out_iqr = []\n",
        "if variable_out_iqr:\n",
        "  out_transf =Winsorizer(capping_method='iqr',tail='both', fold=3,variables = variable_out_iqr)\n",
        "  TrainSet = out_transf.fit_transform(TrainSet)\n",
        "  TestSet = out_transf.transform(TestSet)\n",
        "\n",
        "print(\"* Winsorizer Outlier transformation done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kMYDKU2yuT"
      },
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo7iQbNn2yuX"
      },
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqF5Z5TGrGXB"
      },
      "source": [
        "# from raw numerical variables\n",
        "# variables_engineering = ['MonthlyCharges', 'TotalCharges', 'tenure']\n",
        "# variables_engineering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGfq83p02yuY"
      },
      "source": [
        "* Step 2: Select the engineering transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkiztpOX2yua"
      },
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-pse1Ge2yub"
      },
      "source": [
        "* Step 3: Create a separate dataframe, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hnHWkNf2yub"
      },
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "df_engineering.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJsN5KNe2yuc"
      },
      "source": [
        "* Step 4: Create engineered variables(s) applying the transformation(s), assess engineered variables distribution and select most suitable method for each variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq-jWVT2e2v3"
      },
      "source": [
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6,selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVQW3s0QfasQ"
      },
      "source": [
        "corr_sel.features_to_drop_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib1Nojcs2yud"
      },
      "source": [
        "* Step 5: If you are satisfied, apply the selected transformers to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55pxyEpX2yud"
      },
      "source": [
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6,selection_method=\"variance\")\n",
        "TrainSet = corr_sel.fit_transform(TrainSet)\n",
        "TestSet = corr_sel.transform(TestSet)\n",
        "\n",
        "print(f\"* correlated feature sets: \\n{corr_sel.correlated_feature_sets_}\\n\")\n",
        "print(f\"* features to drop: \\n{corr_sel.features_to_drop_}\\n\")\n",
        "print(\"* SmartCorrelatedSelection transformation done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE0Cn10d4nYP"
      },
      "source": [
        "# Compare Feature Engineered data with collected data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HjlLR3o4tnc"
      },
      "source": [
        "**You are about to start modeling**. \n",
        "  * There is a tradeoff when you clean and engineer your data, since the orginal distribution is changed. \n",
        "\n",
        "The key aspect is to not change in a manner that will jeopardize the modeling.\n",
        "  * But instead, it will help the model to **understand the intrinsic relationship among the variables**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5AFL7K2bEgQ"
      },
      "source": [
        "Create a separate DataFrame with data already engineered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB38CDF15HCB"
      },
      "source": [
        "df_feature_engineering = pd.concat([TrainSet, TestSet], axis=0)\n",
        "df_feature_engineering.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsj4xd1rqKUb"
      },
      "source": [
        "df_feature_engineering.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eXUJB0ebNGt"
      },
      "source": [
        "Load collected data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeBNGJPc5myC"
      },
      "source": [
        "df_raw = pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\")\n",
        "df_raw.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RryFxqnx5-ZF"
      },
      "source": [
        "Quick EDA on both"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSq7q2nu6Awu"
      },
      "source": [
        "# EDA on data after feature engineering\n",
        "from pandas_profiling import ProfileReport\n",
        "ProfileReport(df=df_feature_engineering,minimal=True).to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYF2bCiAdDP0"
      },
      "source": [
        "Compare by \"Mirror cell in tab\" for the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52yfPnsn6A4W"
      },
      "source": [
        "# EDA on raw data\n",
        "ProfileReport(df=df_raw,minimal=True).to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZF7gpo9ecza"
      },
      "source": [
        "Correlation Analysis and PPS analysis on engineered data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Rm77eWec43"
      },
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df_feature_engineering)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EFVgATaeg4z"
      },
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,\n",
        "                  CorrThreshold=0.6, PPS_Threshold=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZiCdjylhGP-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSUo5fxnozU"
      },
      "source": [
        "# So what is the conclusion? :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOP2pVnJnwxS"
      },
      "source": [
        "The list below shows the transformations needed for feature engineering.\n",
        "  * You will add these steps into the ML Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9EtvPIPnpYb"
      },
      "source": [
        "\n",
        "Feature Engineering Transformers\n",
        "  * Ordinal categorical encoding: `['gender', 'Partner', Dependents', 'PhoneService','MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup','DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies','Contract', 'PaperlessBilling', 'PaymentMethod']`\n",
        "  * Smart Correlation Selection: `['OnlineSecurity', 'DeviceProtection', 'TechSupport']`\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fytx9QXgafDW"
      },
      "source": [
        "Save the notebook in your project repo. Then terminate the session (Runtime - Manage Sessions - Terminate)"
      ]
    }
  ]
}