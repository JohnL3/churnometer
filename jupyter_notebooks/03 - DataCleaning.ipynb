{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.12 64-bit"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning Notebook"
      ],
      "metadata": {
        "id": "0aStgWSO0E0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "\n",
        "*   Evaluate missing data\n",
        "*   Clean data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n",
        " \n",
        "\n",
        "  * Drop Variables:  `['customerID', 'TotalCharges' ]`\n",
        "\n"
      ],
      "metadata": {
        "id": "1eLEkw5O0ECa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "kspgyffxear-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change working directory"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to make the parent of current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm the new current directory"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_tdAGw4Zwssu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Collected data"
      ],
      "metadata": {
        "id": "-mavJ8DibrcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "df_raw_path = \"outputs/datasets/collection/TelcoCustomerChurn.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.info()"
      ],
      "outputs": [],
      "metadata": {
        "id": "C2ELZj83tF1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51e2567-7f6c-4200-b9c2-d4982d22e6fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas Profiling"
      ],
      "metadata": {
        "id": "Iue5e5GJ_vZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick EDA"
      ],
      "metadata": {
        "id": "QhWelcb_17dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df=df, minimal=True)\n",
        "profile.to_notebook_iframe()"
      ],
      "outputs": [],
      "metadata": {
        "id": "oyi3gi2-_q1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation and PPS Analysis"
      ],
      "metadata": {
        "id": "LvwabO0JsmYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* supporting functions"
      ],
      "metadata": {
        "id": "qufL2HWrF8ig"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "def heatmap_corr(df,threshold):\n",
        "  if len(df.columns) > 1:\n",
        "    mask = np.zeros_like(df, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    mask[abs(df) < threshold] = True\n",
        "\n",
        "    fig, axes = plt.subplots(figsize=(20,12))\n",
        "    sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                mask=mask, cmap='viridis', annot_kws={\"size\": 8}, ax=axes,\n",
        "                linewidth=0.5\n",
        "                     )\n",
        "    axes.set_yticklabels(df.columns, rotation = 0)\n",
        "    plt.ylim(len(df.columns),0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df,threshold):\n",
        "    if len(df.columns) > 1:\n",
        "\n",
        "      mask = np.zeros_like(df, dtype=np.bool)\n",
        "      mask[abs(df) < threshold] = True\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(20,12))\n",
        "      ax = sns.heatmap(df, annot=True, xticklabels=True,yticklabels=True,\n",
        "                        mask=mask,cmap='rocket_r', annot_kws={\"size\": 8},\n",
        "                       linewidth=0.05,linecolor='grey')\n",
        "      \n",
        "      plt.ylim(len(df.columns),0)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "  df_corr_spearman = df.corr(method=\"spearman\")\n",
        "  df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "  pps_matrix_raw = pps.matrix(df)\n",
        "  pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "  pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "  print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "  print(pps_score_stats.round(3))\n",
        "\n",
        "  return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,CorrThreshold,PPS_Threshold):\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"* Analyze how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "  print(\"* Analyze multi colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "  print(\"It evaluates monotonic relationship \\n\")\n",
        "  heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "  print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "  heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "  print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "  heatmap_pps(df=pps_matrix,threshold=PPS_Threshold)"
      ],
      "outputs": [],
      "metadata": {
        "id": "g6Zy_MglsmYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Calculate Correlations and Power Predictive Score"
      ],
      "metadata": {
        "id": "ryNo1VnXSK9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "3_Z4SXf6GbED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6978dbde-ebb0-4023-af6c-b6f896ced798"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Display at Heatmaps"
      ],
      "metadata": {
        "id": "cJ-0L4PiSPEK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix,\n",
        "                  CorrThreshold=0.2, PPS_Threshold=0.2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ioE3yuC4Q7QK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1946ffa1-2e5d-4972-eeda-45d7fe382314"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "bYdZHyDhu4kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessing Missing Data Levels"
      ],
      "metadata": {
        "id": "HxoVRefhu2Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Custom function to display missing data levels in a dataframe, it shows the aboslute levels, relative levels and data type"
      ],
      "metadata": {
        "id": "pcocWZkIx6nk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def EvaluateMissingData(df):\n",
        "  missing_data_absolute = df.isnull().sum()\n",
        "  missing_data_percentage = round(missing_data_absolute/len(df)*100 , 2)\n",
        "  df_missing_data = (pd.DataFrame(\n",
        "                          data= {\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                 \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                 \"DataType\":df.dtypes}\n",
        "                                  )\n",
        "                    .sort_values(by=['PercentageOfDataset'],ascending=False)\n",
        "                    .query(\"PercentageOfDataset > 0\")\n",
        "                    )\n",
        "\n",
        "  return df_missing_data"
      ],
      "outputs": [],
      "metadata": {
        "id": "z9CoLqBhO7ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check missing data levels for collected dataset"
      ],
      "metadata": {
        "id": "HHrzaG-ZhEKt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "EvaluateMissingData(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zxvNnLAjxCSi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "e6239998-21e1-4401-80de-e25c33bbba52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning Spreadsheet Summary"
      ],
      "metadata": {
        "id": "mWAI2EAp2FNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Go to your spreadsheet at your Google Drive to list **potential approaches to handle missing data**!"
      ],
      "metadata": {
        "id": "Pp-lSdVH-_-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with Missing Data"
      ],
      "metadata": {
        "id": "nsL8yYQEyOSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* It is assumed that you  already:\n",
        "  * assessed the missing data levels, \n",
        "  * did a quick EDA, \n",
        "  * checked correlation (pearson, spearman),\n",
        "  * checked power predictive score.\n",
        "* So you are aware of the variables to work on\n",
        "\n",
        "---\n",
        "\n",
        "* **Strategy**\n",
        "* First, for all variables you need to imput missing data, write potential imputation approach for data cleaning.\n",
        "  * Over the course, you saw multiple approaches for dealing with missing data, like DropVariables, DropNA, Imput with mean/median/mode, Imput the most frequent item etc\n",
        "\n",
        "* Then, you will **iterate the steps below across different imputation approaches**, so at the end you will have dealt with all variables with missing data\n",
        "\n",
        "  * 1 -  Select a **imputation approach**\n",
        "  * 2 - Select **variables** to apply the approach\n",
        "  * 3 - Create a **separate dataframe** applying this imputation approach to the selected variables\n",
        "  * 4 - **Compare** this new dataset with initial dataset to validate/assess the effect on distribution on variables\n",
        "  * 5 - **If** you are satisfied, **apply** the selected imputation approach to the initial dataframe\n",
        "  * 6 - **Evaluate** if you have more variables to deal. If yes, iterate. If not, you are done.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "P_52ePITy6pQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom function"
      ],
      "metadata": {
        "id": "vADd7Ruy2J8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom function to evaluate variables distribution before and after applying a given method. "
      ],
      "metadata": {
        "id": "xrrjpCSCvLz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
        "\n",
        "  flag_count=1 # Indicate plot number\n",
        "  \n",
        "  # distinguish between numerical and categorical variables\n",
        "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
        "\n",
        "  # select variables in which the given data cleaning method was not applied  \n",
        "  variables_not_applied_with_method = [x for x in df_cleaned.columns if x not in variables_applied_with_method]\n",
        "\n",
        "  # scan over variables, \n",
        "    # first on variables that you applied the method\n",
        "    # if variable is numerical, plots histogram, if categorical, plots barplot\n",
        "  for set_of_variables in [variables_applied_with_method,variables_not_applied_with_method]:\n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{set_of_variables} \\n\\n\")\n",
        "  \n",
        "\n",
        "    for var in set_of_variables:\n",
        "      if var in categorical_variables:   \n",
        "        # it is categorical variable: barplot\n",
        "\n",
        "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
        "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
        "        dfAux = pd.concat([df1, df2], axis=0)\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"]).set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.legend() \n",
        "\n",
        "      else: \n",
        "        # it is numerical variable: histogram\n",
        "\n",
        "        # use a statistical test to inform if there is significant change\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\")\n",
        "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\").set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.legend() \n",
        "\n",
        "      plt.show()\n",
        "      flag_count+= 1\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "O_N7yZVnPBqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning Summary"
      ],
      "metadata": {
        "id": "Jt8Yqjy6ghyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "List here the imputation approaches you want initially to try.\n",
        "* Drop\n",
        "\n",
        "\n",
        "**The list above is your guide, your map to know in which stage you are in the data cleaning process**"
      ],
      "metadata": {
        "id": "O4GaYe_DgqwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Train and Test Set"
      ],
      "metadata": {
        "id": "W12Z8KIPoZ-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You have to split train and test set for cleaning the data\n",
        "  * Unless you consider only Drop Variables and Drop Rows, which is not the case.\n",
        "  * Hint: in the majority of the time in the workplace, you will need to split into train and test set"
      ],
      "metadata": {
        "id": "C4T10lOcofen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['Churn'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "Knk7DcVborLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049ef5f2-c431-4a9d-fd87-00809986c38d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ],
      "outputs": [],
      "metadata": {
        "id": "zdMPKBU_yCpF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "df45adaf-5e93-4644-919f-6b82e51fe8b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop Variables\n"
      ],
      "metadata": {
        "id": "EUeGUUmu4qru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hint: you may drop Variables with more than 80% of missing data, since these variables will likely not add much value. However, this is not the case in this dataset\n",
        "* Step 1: imputation approach: **Drop Variables**\n",
        "* Step 2: Select variables to apply the imputation approach\n"
      ],
      "metadata": {
        "id": "cLTFTio14qr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "variables_method = ['customerID', 'TotalCharges' ]\n",
        "\n",
        "print(f\"* {len(variables_method)} variables to drop \\n\\n\"\n",
        "    f\"{variables_method}\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "zlQBeLo44qr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a342431-6be3-48fa-ce83-ed00489d960c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We are dropping `CustomerID`, since it is a unique identifier to each customer, it doesn't add information to the dataset as it is\n",
        "* We are dropping `TotalCharges` since for the context of the ML projects, a prospect doesn't have `TotalCharges`"
      ],
      "metadata": {
        "id": "h1kKajldGrqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Step 3: Create a separate dataframe applying this imputation approach to the selected variables"
      ],
      "metadata": {
        "id": "7au_UdA34qr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "df_method = imputer.transform(TrainSet)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zh9E2cEe4qr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Step 4: Assess the effect on variable's distribution"
      ],
      "metadata": {
        "id": "fsjRXMlU4qr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this case, no effect on variables distribution, since you are not removing rows, but columns\n",
        "* The effect might be losing features that might have a relevant impact in your machine learning model."
      ],
      "metadata": {
        "id": "zc9pg4ww_BO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Step 5: If you are statisfied, apply the imputation approach in your dataframe"
      ],
      "metadata": {
        "id": "uBq1a_Me4qr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "imputer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transform(TestSet)"
      ],
      "outputs": [],
      "metadata": {
        "id": "NGO5M1k44qr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Step 6: Evaluate if you have more variables to deal. If yes, iterate. If not, you are done."
      ],
      "metadata": {
        "id": "LozxNCVO4qr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "EvaluateMissingData(TrainSet)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zGmZy46L4qr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "3597364f-19a3-42af-8d56-3bbcfe8675e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push cleaned data to Repo"
      ],
      "metadata": {
        "id": "eD--GI5Jvp8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/cleaned') # create outputs/datasets/collection folder\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "1SNWONrYwu6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Set"
      ],
      "metadata": {
        "id": "TE3DQfy82r7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\",index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q1ayKT4F2p52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set"
      ],
      "metadata": {
        "id": "BTTabVk-2ulD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\",index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zCjorYny2qCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Push** generated/new files from this Session to GitHub repo"
      ],
      "metadata": {
        "id": "cZcmA1wG8AdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You may now go to \"Push generated/new files from this session to GitHub Repo\" section and push these files to the repo"
      ],
      "metadata": {
        "id": "mMucnZHuD3CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Git status"
      ],
      "metadata": {
        "id": "FUla5863TKyk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "! git status"
      ],
      "outputs": [],
      "metadata": {
        "id": "NzjZgWV-TMOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Git commit"
      ],
      "metadata": {
        "id": "G1kUQ0VIoi4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "CommitMsg = \"add-files-data-cleaning\"\n",
        "!git add .\n",
        "!git commit -m {CommitMsg}"
      ],
      "outputs": [],
      "metadata": {
        "id": "9dafOBor8OoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Git Push"
      ],
      "metadata": {
        "id": "bXkyUs70oloW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!git push origin main"
      ],
      "outputs": [],
      "metadata": {
        "id": "_0NCb8-L8Vr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well done!"
      ],
      "metadata": {
        "id": "8qboMRkGaIqD"
      }
    }
  ]
}