{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar customer behaviour\n",
        "* Understand profile for each cluster\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n",
        "\n",
        "## Additional Comments | Insights | Conclusions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuGQj9lDAEo"
      },
      "source": [
        "# Install and Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx1yVscyB3M"
      },
      "source": [
        "* You eventually will need to restart runtime when installing packages, please note cell output when installing a package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBfLDnhx-2k"
      },
      "source": [
        "! pip install feature-engine==1.0.2\n",
        "! pip install yellowbrick==1.3\n",
        "! pip install scikit-learn==0.24.2\n",
        "\n",
        "# Code for restarting the runtime, that will restart colab session\n",
        "# It is a good practice after you install a package in a colab session\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUFuYskeybZL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0QdOnpiUTRC"
      },
      "source": [
        "# Setup GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIifw4yCpZwI"
      },
      "source": [
        "* Go to Edit â†’ Notebook Settings\n",
        "* In the Hardware accelerator menu, selects GPU\n",
        "* note: when you select an option, either GPU, TPU or None, you switch among kernels/sessions\n",
        "\n",
        "---\n",
        "* How to know if I am using the GPU?\n",
        "  * run the code below, if the output is different than '0' or null/nothing, you are using GPU in this session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJJd1XhUTjd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHiVgPviuMx"
      },
      "source": [
        "# **Connection between: Colab Session and your GitHub Repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtPQ7EnPiuMy"
      },
      "source": [
        "### Insert your **credentials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhDHrzxEiuMz"
      },
      "source": [
        "* The variable's content will exist only while the session exists. Once this session terminates, the variable's content will be erased permanently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye8aYwLkiuMz"
      },
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from IPython.display import clear_output \n",
        "\n",
        "print(\"=== Insert your credentials === \\nType in and hit Enter\")\n",
        "os.environ['UserName'] = getpass('GitHub User Name: ')\n",
        "os.environ['UserEmail'] = getpass('GitHub User E-mail: ')\n",
        "os.environ['RepoName'] = getpass('GitHub Repository Name: ')\n",
        "os.environ['UserPwd'] = getpass('GitHub Account Token: ')\n",
        "clear_output()\n",
        "print(\"* Thanks for inserting your credentials!\")\n",
        "print(f\"* You may now Clone your Repo to this Session, \"\n",
        "      f\"then Connect this Session to your Repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JjRkDt1eOAr"
      },
      "source": [
        "* **Credentials format disclaimer**: when opening Jupyter notebooks in Colab that are hosted at GitHub, we ask you to not consider special characters in your **password**, like @ ! \" # $ % & ' ( ) * + , - . / :;< = > ? @ [\\ ]^_ ` { } | ~\n",
        "  * Otherwise it will not work properly the git push command, since the credentials are concatenated in the command: username:password@github.com/username/repo , the git push command will not work properly when these terms have special characters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_amd2ygiuM0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I2eQe-YiuM0"
      },
      "source": [
        "### **Clone** your GitHub Repo to your current Colab session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfD0o1u1iuM0"
      },
      "source": [
        "* So you can have access to your project's files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGOPTqcmiuM1"
      },
      "source": [
        "! git clone https://github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "! rm -rf sample_data   # remove content/sample_data folder, since we dont need it for this project\n",
        "\n",
        "import os\n",
        "if os.path.isdir(os.environ['RepoName']):\n",
        "  print(\"\\n\")\n",
        "  %cd /content/{os.environ['RepoName']}\n",
        "  print(f\"\\n\\n* Current session directory is:{os.getcwd()}\")\n",
        "  print(f\"* You may refresh the session folder to access {os.environ['RepoName']} folder.\")\n",
        "else:\n",
        "  print(f\"\\n* The Repo {os.environ['UserName']}/{os.environ['RepoName']} was not cloned.\"\n",
        "        f\" Please check your Credentials: UserName and RepoName\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uhzcFjeiuM1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS3yEKFJiuM1"
      },
      "source": [
        "### **Connect** this Colab session to your GitHub Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tFmsYgiuM2"
      },
      "source": [
        "* So if you need, you can push files generated in this session to your Repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveMisgfiuM2"
      },
      "source": [
        "! git config --global user.email {os.environ['UserEmail']}\n",
        "! git config --global user.name {os.environ['UserName']}\n",
        "! git remote rm origin\n",
        "! git remote add origin https://{os.environ['UserName']}:{os.environ['UserPwd']}@github.com/{os.environ['UserName']}/{os.environ['RepoName']}.git\n",
        "\n",
        "# the logic is: create a temporary file in the sessions, update the repo. Delete this file, update the repo\n",
        "# If it works, it is a signed that the session is connected to the repo.\n",
        "import uuid\n",
        "file_name = \"session_connection_test_\" + str(uuid.uuid4()) # generates a unique file name\n",
        "with open(f\"{file_name}.txt\", \"w\") as file: file.write(\"text\")\n",
        "print(\"=== Testing Session Connectivity to the Repo === \\n\")\n",
        "! git add . ; ! git commit -m {file_name + \"_added_file\"} ; ! git push origin main \n",
        "print(\"\\n\\n\")\n",
        "os.remove(f\"{file_name}.txt\")\n",
        "! git add . ; ! git commit -m {file_name + \"_removed_file\"}; ! git push origin main\n",
        "\n",
        "# delete your Credentials (username and password)\n",
        "os.environ['UserName'] = os.environ['UserPwd'] = os.environ['UserEmail'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKKIufOcexSz"
      },
      "source": [
        "* If output above indicates there was a **failure in the authentication**, please insert again your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data for Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\")\n",
        "      .drop(['customerID', 'TotalCharges', 'Churn', 'tenure' ],axis=1) \n",
        ")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Cluster Pipeline considering all data: KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "### PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "       \n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  variables = [ 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
        "                                                               'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                                               'OnlineBackup','DeviceProtection', 'TechSupport', \n",
        "                                                               'StreamingTV', 'StreamingMovies','Contract', \n",
        "                                                               'PaperlessBilling', 'PaymentMethod'])\n",
        "      ),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrxSzwiJGzFP"
      },
      "source": [
        "## ML Pipeline for Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNJCrTiClFR"
      },
      "source": [
        "where `n_components` of PCA and `n_clusters` of KMeans will be updated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU953c8ZGzUQ"
      },
      "source": [
        "def PipelineCluster():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "  pipe.steps.append([\"PCA\",PCA(n_components=3, random_state=0)])\n",
        "  pipe.steps.append([\"scaler\",StandardScaler()])\n",
        "  pipe.steps.append([\"model\",KMeans(n_clusters=4, random_state=0)])\n",
        "  return pipe\n",
        "\n",
        "PipelineCluster()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzF21hNlG3H9"
      },
      "source": [
        "## ML Pipeline for a classifier to explain the clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5m15PdrBzFE"
      },
      "source": [
        "We are considering a model that typically offers good results and features importance can be assessed with `.features_importance_`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55qpqXLaG3d9"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "   pipe.steps.append([\"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=0))])\n",
        "   pipe.steps.append([\"scaler\",StandardScaler()])\n",
        "   pipe.steps.append([\"model\",GradientBoostingClassifier(random_state=0)])\n",
        "   return pipe\n",
        "  \n",
        "PipelineClf2ExplainClusters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrr31sD9DyvY"
      },
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC9WunDaCuQo"
      },
      "source": [
        "Apply PCA separately to find the most suitable `n_components`, update the value on ML Pipeline for Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1fnArcWET0D"
      },
      "source": [
        "It needs the dataset after data cleaning and feature engineering\n",
        "  * That means you have to remove 3 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es49S65qqvRw"
      },
      "source": [
        "pipeline_pca = PipelineDataCleaningAndFeatureEngineering()\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "print(df_pca.shape)\n",
        "df_pca.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlABEj9Iw6Jr"
      },
      "source": [
        "Apply PCA component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM_Xsqxsrt5M"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components = 3\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFC6gDrfq83G"
      },
      "source": [
        "Heatmap: PCA components and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plQLu3Z3tE5Y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "df_comp = pd.DataFrame(pca.components_, columns=df_pca.columns)\n",
        "plt.figure(figsize=(20,5))\n",
        "sns.heatmap(df_comp,center=0,linewidths=.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybShC2CGHNB0"
      },
      "source": [
        "Major Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7AalhG0QOor"
      },
      "source": [
        "major_pca_variables = df_comp[abs(df_comp) > 0.1].dropna(axis=1, how='all').columns.to_list()\n",
        "print(f\"* There are {len(major_pca_variables)} major variables in the PCA: \\n{major_pca_variables}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9NtDj4EtEJ"
      },
      "source": [
        "## Elbow Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txwWrETeDFS2"
      },
      "source": [
        "Find the most suitable `n_clusters`, update the value on ML Pipeline for Cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXb_H6iGx6U"
      },
      "source": [
        "Prepare data for analysis\n",
        "  * You need to clean and feature engineer your data using the pipeline without the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVaMnb9vGyBw"
      },
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_elbow = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_elbow = pipeline_elbow.fit_transform(df)\n",
        "\n",
        "print(df_elbow.shape,'\\n')\n",
        "df_elbow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1KLQujEvgi"
      },
      "source": [
        "Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZBcHjt7EwFT"
      },
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,16))\n",
        "visualizer.fit(df_elbow) \n",
        "visualizer.show() \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "## Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "Quick recap in our raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "source": [
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "source": [
        "X = df.copy()\n",
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AU1E4IdkyLd"
      },
      "source": [
        "Cluster model output is an array with clusters labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9lx5EqalOtv"
      },
      "source": [
        "pipeline_cluster['model'].labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvr_dnl6wiE_"
      },
      "source": [
        "Let's check its shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRk0uyz1lSIW"
      },
      "source": [
        "pipeline_cluster['model'].labels_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0iMkjJHXSI"
      },
      "source": [
        "## Add cluster labels to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJzUxAmkv07"
      },
      "source": [
        "The goal is to merge cluster labels to `X` DataFrame. There is one attention point: **does the pipeline have a step for dropping rows?** \n",
        "  * If yes, before merging, we need to drop these rows from X. the code below can do that\n",
        "\n",
        "```\n",
        "drop_imputer = DropMissingData(variables =['place here the variables where you drop rows'])\n",
        "X = drop_imputer.fit_transform(X)\n",
        "```\n",
        "* If no, ignore this step.\n",
        "\n",
        "Our project doesn't need this step. You can confirm that comparing the length of `X` and the length of `cluster label predictions`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAUY6-_6A-rN"
      },
      "source": [
        "print(X.shape)\n",
        "print(pipeline_cluster['model'].labels_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "We add a column \"`Clusters`\" to the data and check clusters distribution\n",
        "* Clusters don't look to be imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVeCKYbRW9Xi"
      },
      "source": [
        "This is how our data look like from now\n",
        "  * Check the last column: `Clusters`\n",
        "  * Quick reminder: The data is unprocessed (**no data cleaning or feature engineering applied yet**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "source": [
        "print(X.shape)\n",
        "X.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnjHhYjXng2r"
      },
      "source": [
        "Here we are saving the cluster predictions for this pipeline to use in a fututre moment. We will get back to that soon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWgb0kPOWtMa"
      },
      "source": [
        "cluster_anwers_with_all_variables = X['Clusters']\n",
        "cluster_anwers_with_all_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "## Evaluate Clusters silhouette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJj6IZqmrxx"
      },
      "source": [
        "To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)\n",
        "  * clusters arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4j4h-2Shlq4"
      },
      "source": [
        "pipeline_silhouette = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_transformed = pipeline_silhouette.transform(df)\n",
        "df_transformed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSSZZSBEXPD7"
      },
      "source": [
        "Silhouette plot function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqxawC8lq1R"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "def EvaluateClusterSilhouette(X,n_clusters,cluster_labels):\n",
        "\n",
        "  print(f\"  * The silhouette score range is -1 to +1, where: \\n\"\n",
        "        f\"  - 1 means the clusters are dense and properly separated.\\n\"\n",
        "        f\"  - 0 means the clusters are overlapping. \\n\"\n",
        "        f\"  - A negative score means that these data points from that cluster may be wrong, \"\n",
        "        f\"they should belong to other cluster.\")  \n",
        "  \n",
        "  print(f\"\\n* You should evaluate:\\n\"\n",
        "      \"  * If there are clusters with below average silhouette scores. \\n\"\n",
        "      \"  * If there is broad variation in the silhouette plots's size across clusters. \\n\"\n",
        "      \"  * If the thickness of the silhouettes are uniform/similar in general \\n\")\n",
        "  \n",
        "  fig = plot_clusters_silhouette(X,n_clusters,cluster_labels)\n",
        "  plt.show()\n",
        "\n",
        "def plot_clusters_silhouette(X,n_clusters,cluster_labels):\n",
        "\n",
        "  silhouette_avg = silhouette_score(X, cluster_labels,random_state=0)\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,7))\n",
        "  axes.set_xlim([-0.1, 1])\n",
        "  axes.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "  sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "  y_lower = 10\n",
        "  for i in range(n_clusters):\n",
        "    ith_cluster_silhouette_values = \\\n",
        "      sample_silhouette_values[cluster_labels == i]\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "    axes.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "              0, ith_cluster_silhouette_values,\n",
        "              facecolor=color, edgecolor=color, alpha=0.7)\n",
        "    axes.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "  axes.set_title(\"The silhouette plot for each cluster\")\n",
        "  axes.set_xlabel(\"The silhouette coefficient values\")\n",
        "  axes.set_ylabel(\"Cluster label\")\n",
        "  axes.text(x=silhouette_avg*1.01, y=len(X)*0.95, s=f\"Silhouette Average: {round(silhouette_avg,2)}\", fontsize=12, c='r')\n",
        "  axes.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "  axes.set_yticks([])\n",
        "  axes.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "  return fig\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpimuOdgXS5G"
      },
      "source": [
        "Evaluate Cluster Silhouette"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsrpxXqljJ-y"
      },
      "source": [
        "EvaluateClusterSilhouette(\n",
        "    X=df_transformed,\n",
        "    n_clusters=X['Clusters'].nunique(),\n",
        "    cluster_labels=X['Clusters'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "## Fit a classifier, where target is cluster labels and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipsJSzhhkUiu"
      },
      "source": [
        "We are in a moment where we have predictions from the cluster pipeline, but we don't have a meaning for them yet. \n",
        "* We seek to understand clusters' profile **bold text**\n",
        "\n",
        "We need to find the most relevant variables, to define each cluster in terms of each relevant variable\n",
        "* Our new dataset has `Clusters`, which will be the **target for a classifier**. The most relevant features for this classifier, will be the most relevant variables when we run a classifier where the target is the cluster labels!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHTHwMOoz5le"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6sGUn0XyDm"
      },
      "source": [
        "We copy `X` to a DataFrame `df_clf`, just to separate the cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Cjsn6l5cqF"
      },
      "source": [
        "Fit pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QPwTqa0OcY"
      },
      "source": [
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "## Assess Most Important Features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG5ztHxsKcd5"
      },
      "source": [
        "# after data cleaning and feat engine, the feature space changes\n",
        "columns_after_data_cleaning_feat_eng = (PipelineDataCleaningAndFeatureEngineering()\n",
        "                                        .fit_transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "          'Attribute': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "          'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "  .sort_values(by='Importance', ascending=False)\n",
        "  )\n",
        "\n",
        "best_features = df_feature_importance['Attribute'].to_list() # reassign best features in importance order\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar',x='Attribute',y='Importance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgul0EF9nx_E"
      },
      "source": [
        "We will store the best_features for future usage. We will get back to that soon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzyMkwHznyG8"
      },
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NBCTceJbgRu"
      },
      "source": [
        "Custom Functions for Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "* Table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "source": [
        "def Clusters_IndividualDescription(EDA_Cluster,cluster):\n",
        "\n",
        "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "  for col in EDA_Cluster.columns:\n",
        "    \n",
        "    try:  # eventually a given cluster will have only mssing data for a given variable\n",
        "      \n",
        "      if EDA_Cluster[col].dtypes == 'object':\n",
        "        \n",
        "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "        Description = ''\n",
        "        \n",
        "        for x in range(len(top_frequencies)):\n",
        "          freq = top_frequencies.iloc[x]\n",
        "          category = top_frequencies.index[x][0]\n",
        "          CategoryPercentage = int(round(freq*100,0))\n",
        "          statement =  f\"'{category}': {CategoryPercentage}% , \"  \n",
        "          Description = Description + statement\n",
        "        \n",
        "        ClustersDescription.at[0,col] = Description[:-2]\n",
        "\n",
        "\n",
        "      \n",
        "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "        Q1 = int(round(DescStats.iloc[4,0],0))\n",
        "        Q3 = int(round(DescStats.iloc[6,0],0))\n",
        "        Description = f\"{Q1} -- {Q3}\"\n",
        "        ClustersDescription.at[0,col] = Description\n",
        "    \n",
        "    \n",
        "    except Exception as e:\n",
        "      ClustersDescription.at[0,col] = 'Not available'\n",
        "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "  \n",
        "  ClustersDescription['Cluster'] = str(cluster)\n",
        "  \n",
        "  return ClustersDescription\n",
        "\n",
        "\n",
        "def DescriptionAllClusters(df_cluster_profile):\n",
        "\n",
        "  DescriptionAllClusters = pd.DataFrame(columns=df_cluster_profile.drop(['Clusters'],axis=1).columns)\n",
        "  for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "    \n",
        "      EDA_ClusterSubset = df_cluster_profile.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
        "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster)\n",
        "      DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
        "\n",
        "  \n",
        "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
        "  return DescriptionAllClusters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHo7wmH68AYc"
      },
      "source": [
        "* Cluster distribution per Variable (absolute and relative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN23X2dT8AeA"
      },
      "source": [
        "import plotly.express as px\n",
        "def cluster_distribution_per_variable(df,target):\n",
        "\n",
        "\n",
        "  df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index() \n",
        "  df_bar_plot.columns = ['Clusters',target,'Count']\n",
        "  df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "  print(f\"Clusters distribution across {target} levels\")\n",
        "  fig = px.bar(df_bar_plot, x='Clusters',y='Count',color=target,width=800, height=500)\n",
        "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "  df_relative = (df\n",
        "                 .groupby([\"Clusters\", target])\n",
        "                 .size()\n",
        "                 .groupby(level=0)\n",
        "                 .apply(lambda x:  100*x / x.sum())\n",
        "                 .reset_index()\n",
        "                 .sort_values(by=['Clusters'])\n",
        "                 )\n",
        "  df_relative.columns = ['Clusters',target,'Relative Percentage (%)']\n",
        " \n",
        "\n",
        "  print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "  fig = px.line(df_relative, x='Clusters',y='Relative Percentage (%)',color=target,width=800, height=500)\n",
        "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
        "  fig.update_traces(mode='markers+lines')\n",
        "  fig.show()\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou5ymnDkJJTl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "We will study the profile for the main variables that define a cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "df_cluster_profile.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mfJRrFc7wzu"
      },
      "source": [
        "Load Churn levels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSRSNqiF4mnm"
      },
      "source": [
        "df_churn = pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\").filter(['Churn'])\n",
        "df_churn['Churn'] = df_churn['Churn'].astype('object')\n",
        "df_churn.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD0Y3NdJOhm"
      },
      "source": [
        "### Cluster profile on most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0dCJG1zG13D"
      },
      "source": [
        "Considering `df_cluster_profile` and `df_churn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(pd.concat([df_cluster_profile,df_churn], axis=1))\n",
        "clusters_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SS6CCCb74lH"
      },
      "source": [
        "### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwEUdPI2NHOb"
      },
      "source": [
        "df_cluster_vs_churn=  df_churn.copy()\n",
        "df_cluster_vs_churn['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEfOvjx8ZhAH"
      },
      "source": [
        "# Fit New Cluster Pipeline only on most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFno4XnSZlZV"
      },
      "source": [
        "In order to reduce feature space, we will study the trade-off between considering the previous Cluster Pipeline (fitted with all variables) and creating a new Cluster Pipeline with the variables that are most important to define the clusters from previous pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROH8cre2PHWx"
      },
      "source": [
        "best_features_pipeline_all_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLOL2zr4Jr68"
      },
      "source": [
        "## Define trade-off and metrics to compare new and previous Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJyxkSowZ9Cm"
      },
      "source": [
        "To evaluate this tradeoff we will\n",
        "1. Conduct a PCA analysis, with the same amount of components from \n",
        "previous study, in a dataset only with `best_features_pipeline_all_variables` and see if all variables are relevant\n",
        "2. Conduct a elbow study and check if the same number of clusters is suggested\n",
        "3. Fit new cluster pipeline and compare if the both clusters predictions are \"equivalent\"\n",
        "4. Compare silhoutte score\n",
        "5. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar\n",
        "6. Check if the most important features for the classifier are the same.\n",
        "7. Compare if the cluster profile from both cases are \"equivalent\"\n",
        "\n",
        "If we are happy to say **yes** for them, you can use a cluster pipeline with reduced feature space!\n",
        "* The **gain** is that in real time (which is the major purpose of Machine Learning) you will need less variables for running predictions and decision making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxyTpm4EJx4s"
      },
      "source": [
        "## Consider the data with the most relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQu9033oZ6r9"
      },
      "source": [
        "df_reduced = df.filter(best_features_pipeline_all_variables)\n",
        "df_reduced.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ub9_YoIeaS5"
      },
      "source": [
        "## Rewrite ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrlQuieZeaS6"
      },
      "source": [
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "        # we updated the pipeline, considering only the new variables      \n",
        "       (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  variables = ['PaymentMethod', 'InternetService',\n",
        "                                                               'DeviceProtection','OnlineSecurity'])\n",
        "      ),\n",
        "\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4JsUASTfCiE"
      },
      "source": [
        "## Apply PCA and compare to previous PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJs6VtKvfCiS"
      },
      "source": [
        "It needs the dataset after data cleaning and feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ09ogO8fCiS"
      },
      "source": [
        "pipeline_pca = PipelineDataCleaningAndFeatureEngineering()\n",
        "df_pca = pipeline_pca.fit_transform(df_reduced)\n",
        "print(df_pca.shape)\n",
        "df_pca.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grI6_LgYfCiU"
      },
      "source": [
        "Apply PCA component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmvyqKRFfOsH"
      },
      "source": [
        "print(f\"* n_components here should be the same amount from previous study: {n_components}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upco-DU-fCiU"
      },
      "source": [
        "n_components = 3\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjEfQ8F6fCiV"
      },
      "source": [
        "Heatmap: PCA components and variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRPLuEeDfCiV"
      },
      "source": [
        "df_comp = pd.DataFrame(pca.components_, columns=df_pca.columns)\n",
        "plt.figure(figsize=(20,5))\n",
        "sns.heatmap(df_comp, center=0, linewidths=.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhcWphBXJ-xD"
      },
      "source": [
        "Major PCA variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l53tSYX5fCiV"
      },
      "source": [
        "major_pca_variables = df_comp[abs(df_comp) > 0.1].dropna(axis=1, how='all').columns.to_list()\n",
        "print(f\"* There are {len(major_pca_variables)} major variables in the PCA: \\n{major_pca_variables}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBrtfzIufwdW"
      },
      "source": [
        "Note that all variables from `best_features_pipeline_all_variables` are indicated as relevant after applying PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57ncdQ7hBXe"
      },
      "source": [
        "## Apply Elbow analysis and compare to previous Elbow analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Iw99sEhBXr"
      },
      "source": [
        "Prepare data for analysis\n",
        "  * You need to clean and feature engineer your data using the pipeline without the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0wqQOM3hBXr"
      },
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_elbow = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_elbow = pipeline_elbow.fit_transform(df_reduced)\n",
        "\n",
        "print(df_elbow.shape,'\\n')\n",
        "df_elbow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_1H05FKhBXs"
      },
      "source": [
        "Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsAJW4s0hBXt"
      },
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,16))\n",
        "visualizer.fit(df_elbow) \n",
        "visualizer.show() \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFVtLmU5hWyn"
      },
      "source": [
        "The same number of clusters is suggested! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_T1gtprhe8W"
      },
      "source": [
        "## Fit New Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEtzurhOhe8W"
      },
      "source": [
        "Quick recap in our raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZRR3wChe8X"
      },
      "source": [
        "print(df_reduced.shape)\n",
        "df_reduced.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-z3ST3nhe8Z"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAaEPy3Uhe8Z"
      },
      "source": [
        "X = df_reduced.copy()\n",
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Ruqu_vhe8a"
      },
      "source": [
        "Cluster model output is an array with clusters labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ0Klt1phe8a"
      },
      "source": [
        "pipeline_cluster['model'].labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11AZ1w3bhe8a"
      },
      "source": [
        "pipeline_cluster['model'].labels_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pdrt5bdKLDF"
      },
      "source": [
        "## Add cluster labels to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdFwZk1ihe8b"
      },
      "source": [
        "We add a column \"Cluster\" to the data and check clusters distribution\n",
        "* Clusters don't look to be imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUwR8vCEhe8b"
      },
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUxjeMypKOUe"
      },
      "source": [
        "## Compare current cluster labels to previous cluster labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6721kuGiII6"
      },
      "source": [
        "We just fitted a new cluster pipeline and want to compare if its predictions are \"equivalent\" from the previous cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAS2dDXQhe8c"
      },
      "source": [
        "These are the predictions from **previous** cluster pipeline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbVaUAABhe8c"
      },
      "source": [
        "cluster_anwers_with_all_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUiHzLIwimaD"
      },
      "source": [
        "And these are the predictions from **current** cluster pipeline (trained with `df_reduced`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kC7cKKCiwD5"
      },
      "source": [
        "cluster_anwers_with_major_variables = X['Clusters'] \n",
        "cluster_anwers_with_major_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgkF8MyyAjTF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jt-2n1GidBa"
      },
      "source": [
        "We use a confusion matrix to evaluate if the predictions of both pipelines are **\"equivalent\"**\n",
        "* We say equivalent in quotes, because we can't expect that a cluster label 0 in the previous cluster will have the same label in the current cluster. Eventually label 0 in previous cluster pipeline will be a different label in current cluster pipeline\n",
        "* When we reach this **equivalence**, it means both clusters \"clustered\" in a similar way but with different labels. And this is fine, since the label itself doesn't have meaning. We will look for the meaning after. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLy37N1TiiSx"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(cluster_anwers_with_all_variables,\n",
        "                       cluster_anwers_with_major_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXrL1GUnjz1t"
      },
      "source": [
        "We see that one pipeline labeled 1526 observations as label 2 and other as label 0. \n",
        "  * So what is label 2 in the first, is label 0 in the second\n",
        "\n",
        "We also see one pipeline labeled 1674 observations as label 0 and other as label 2.\n",
        "  * So what is label 0 in one, is label 2 in the other\n",
        "  * We notice 146 data points were labelled as 0 in one, and a 1 in the other. This is fine, since is a minority compared to the 1674\n",
        "\n",
        "When you keep comparing you will notice:\n",
        "  * What is label 1 in one, is label 3 in the other\n",
        "  * What is label 3 is one, is label 1 in the other\n",
        "\n",
        "\n",
        "Conclusion\n",
        "* We see that both Clusters Pipelines are not predicting the data 100% equivalent, since few data points have different meaning for each pipeline. \n",
        "* However this is fine, and we say yes for this criteria. This is part of the trade-off we are up to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXSU4Tk3mIBv"
      },
      "source": [
        "## Evaluate current Clusters silhouette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fwxo2fPmIB8"
      },
      "source": [
        "* To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)\n",
        "  * clusters arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IupILQzmIB9"
      },
      "source": [
        "pipeline_silhouette = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_transformed = pipeline_silhouette.transform(df_reduced)\n",
        "df_transformed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozvP79HimICA"
      },
      "source": [
        "Evaluate Cluster Silhouette"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVwnguomICA"
      },
      "source": [
        "EvaluateClusterSilhouette(\n",
        "    X=df_transformed,\n",
        "    n_clusters=X['Clusters'].nunique(),\n",
        "    cluster_labels=X['Clusters'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h98ivtaamYAx"
      },
      "source": [
        "The silhoutte score from both pipelines are similar! Now it has even increased a bit :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcPfalkwmomc"
      },
      "source": [
        "## Rewrite ML Pipeline for a classifier to explain the clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIcOSR42momd"
      },
      "source": [
        "We want again to explain the major variables for our clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZClC5E5mome"
      },
      "source": [
        "We copy `X` to a DataFrame `df_clf`, just to separate the concerns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XoL6tuRmomf"
      },
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSfwpL-Fmomf"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPyXs27Kmomf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W-cV2ts8A_N"
      },
      "source": [
        "Rewrite pipeline to explain clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm63GRYP8BIV"
      },
      "source": [
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "   # no feature selection step\n",
        "   pipe.steps.append([\"scaler\",StandardScaler()])\n",
        "   pipe.steps.append([\"model\",GradientBoostingClassifier(random_state=0)])\n",
        "   return pipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU6mwsFYmomg"
      },
      "source": [
        "Create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3liI7qjmomg"
      },
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkaBhgjOmomg"
      },
      "source": [
        "## Fit a classifier, where target is cluster labels and features remaining variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbmUqfhjmomg"
      },
      "source": [
        "pipeline_clf_cluster.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCk6Swrmomh"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjXpF0x8momh"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Obn_Hcmomh"
      },
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-KlCS6MnBLk"
      },
      "source": [
        "The performance on Train and Test sets are similar, comparing to the previous pipeline! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G07251XWmomh"
      },
      "source": [
        "## Assess Most Important Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE0E0QkbKv-Z"
      },
      "source": [
        "They help the most to define a cluster, compare with previous pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMTUNBYN8fyf"
      },
      "source": [
        "best_features = X_train.columns.to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Attribute': X_train.columns,\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        ".sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "best_features = df_feature_importance['Attribute'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Attribute'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar',x='Attribute',y='Importance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtVUleNhndTp"
      },
      "source": [
        "The most relevant variables, in descending order, to explain the cluster from the previous pipeline are"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuud7SmsnjB9"
      },
      "source": [
        "best_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F47JdIk6njMV"
      },
      "source": [
        "We noticed the features importance order are the same from the previous cluster pipeline :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q1TJSqdI6xK"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8EMIqE5I6xP"
      },
      "source": [
        "We will study the profile for the main variables that define a cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEg92vdnI6xP"
      },
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "df_cluster_profile.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "789CeA0WI6xQ"
      },
      "source": [
        "Load Churn levels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx335aqtI6xR"
      },
      "source": [
        "df_churn = pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\").filter(['Churn'])\n",
        "df_churn['Churn'] = df_churn['Churn'].astype('object')\n",
        "df_churn.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-7jYzhtI6xR"
      },
      "source": [
        "### Cluster profile on most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yi0fwrTI6xR"
      },
      "source": [
        "Considering `df_cluster_profile` and `df_churn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urBmw5HJI6xS"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(pd.concat([df_cluster_profile,df_churn], axis=1))\n",
        "clusters_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDJRuBBgI6xS"
      },
      "source": [
        "### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjMnAqYKI6xS"
      },
      "source": [
        "df_cluster_vs_churn=  df_churn.copy()\n",
        "df_cluster_vs_churn['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xhPLbC4dwXL"
      },
      "source": [
        "## Which pipeline should I keep?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8qASh5k1jph"
      },
      "source": [
        "Let's recap the criteria we consider to evaluate the **trade-off**\n",
        "1. Conduct a PCA analysis, with the same amount of components from previous study, in a dataset only with `best_features_pipeline_all_variables` and see if all variables are relevant\n",
        "2. Conduct a elbow study and check if the same number of clusters is suggested\n",
        "3. Fit new cluster pipeline and compare if the both clusters predictions are \"equivalent\"\n",
        "4. Compare silhoutte score\n",
        "5. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar\n",
        "6. Check if the most important features for the classifier are the same.\n",
        "7. Compare if the cluster profile from both cases are \"equivalent\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPbS7JYN101K"
      },
      "source": [
        "We are happy with all criteria above for the new Cluster Pipeline\n",
        "\n",
        "\n",
        "Now we face a moment of trade-off, where there is no 100% right or wrong decision, it is more a contextual decision\n",
        "* All 7 criteria support the second pipeline. In addition, there is a great gain to have less variables for predicting live data\n",
        "* Therefore we have positive evidence to take the pipeline with less variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4HsuSuqd0g_"
      },
      "source": [
        "pipeline_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxpjktKd1n6"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i9X1oOORAQc"
      },
      "source": [
        "\n",
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ySBIrV1Q4cY"
      },
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y9-0fisd5cl"
      },
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfv9k5xMd7fv"
      },
      "source": [
        "pipeline_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsphnIR84hJ4"
      },
      "source": [
        "joblib.dump(value=pipeline_cluster ,\n",
        "            filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ORnkwG6d74O"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqcwHaVwd9Ff"
      },
      "source": [
        "print(df_reduced.shape)\n",
        "df_reduced.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M26MiJ9Y485Q"
      },
      "source": [
        "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_lcQVXaV0p"
      },
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9datNfsLCVV"
      },
      "source": [
        "These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYeoH7fjaV8J"
      },
      "source": [
        "df_feature_importance.plot(kind='bar',x='Attribute',y='Importance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr0cVVQsaZqk"
      },
      "source": [
        "df_feature_importance.plot(kind='bar',x='Attribute',y='Importance')\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3Z5ivNd9mw"
      },
      "source": [
        "## Cluster Profile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw-mEnI8d_Bv"
      },
      "source": [
        "clusters_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G5CsAl738p7"
      },
      "source": [
        "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RObeac1HQq5a"
      },
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf6p3MN-R7Qc"
      },
      "source": [
        "plot_clusters_silhouette(\n",
        "    X=df_transformed,\n",
        "    n_clusters=X['Clusters'].nunique(),\n",
        "    cluster_labels=X['Clusters'].values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQowyaTAQssB"
      },
      "source": [
        "fig = plot_clusters_silhouette(\n",
        "    X=df_transformed,\n",
        "    n_clusters=X['Clusters'].nunique(),\n",
        "    cluster_labels=X['Clusters'].values)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDr9NoI7LLKF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "257gMsNhiuM3"
      },
      "source": [
        "## **Push** generated/new files from this Session to GitHub repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBtt-lvYLLi8"
      },
      "source": [
        "You can push the files now to the Repo!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUla5863TKyk"
      },
      "source": [
        "* Git status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjZgWV-TMOB"
      },
      "source": [
        "! git status"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_xeleqiuM4"
      },
      "source": [
        "* Git commit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFefbLXiuM4"
      },
      "source": [
        "CommitMsg = \"added-files-cluster-analysis\"\n",
        "! git add .\n",
        "! git commit -m {CommitMsg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFKrJ6fiuM5"
      },
      "source": [
        "* Git Push"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZxYGf_yiuM5"
      },
      "source": [
        "! git push origin main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeIEUrWkJ-6_"
      },
      "source": [
        "Good job, now save the notebook in your repo and terminate the session (Runtime - Manage Session - Terminate)"
      ]
    }
  ]
}