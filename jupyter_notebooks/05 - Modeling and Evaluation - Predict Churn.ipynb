{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Modeling and Evaluation - Predict Customer Churn.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "625c31d6b4db3d7e7e2853cc30dc2062e1cda684f3e49d5f899ae496ae755fe0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "0aStgWSO0E0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a classification model to predict if a prospect will churn or not.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train set (features and target)\n",
        "* Test set (features and target)\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* features importance plot\n"
      ],
      "metadata": {
        "id": "1eLEkw5O0ECa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "9uWZXH9LwoQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change working directory"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to make the parent of the parent of current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm the new current directory"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "OSpFreVRiuM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data For Modelling"
      ],
      "metadata": {
        "id": "-mavJ8DibrcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\")\n",
        "      .drop(labels=['tenure','customerID','TotalCharges'],axis=1)  \n",
        "                    # target variable for regressor, remove from classifier  \n",
        "                    # drop other variables we will not need for this project\n",
        "  )\n",
        "\n",
        "df.info()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know already in upfront that **Train Set Target (Churn) is imbalanced**\n",
        "  * We will apply SMOTE technique to handle that. That was covered in Develop & Deploy an AI System - Target Imbalance\n",
        "  * Therefore, we will produce 2 ML Pipelines:\n",
        "    * One for Data Cleaning and Feature Engineering\n",
        "    * Another for Feature Scaling, Feature Selection and Modeling\n",
        "  * The pipelines will be used to train the pipeline, to test the pipeline and to predict on live data"
      ],
      "metadata": {
        "id": "HJ4IW652pgZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Ofil7xTpm6l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Pipeline with all available data: Sklearn"
      ],
      "metadata": {
        "id": "krjAk78Tbyhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML pipeline for Data Cleaning and Feature Engineering"
      ],
      "metadata": {
        "id": "FfCsXhBYVBJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load Estimators for pipelines"
      ],
      "metadata": {
        "id": "OPskM3Rt6BcM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nk1RSVSYVBWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Data Cleaninig And Feature Engineering"
      ],
      "metadata": {
        "id": "NZWZHhpYaDjf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary', \n",
        "                                                  variables = [ 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
        "                                                               'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                                               'OnlineBackup','DeviceProtection', 'TechSupport', \n",
        "                                                               'StreamingTV', 'StreamingMovies','Contract', \n",
        "                                                               'PaperlessBilling', 'PaymentMethod'])\n",
        "      ),\n",
        "       \n",
        "      (\"SmartCorrelatedSelection\",SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                           threshold=0.6, selection_method=\"variance\")\n",
        "      ),\n",
        "       \n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base"
      ],
      "outputs": [],
      "metadata": {
        "id": "C6keis6ao8LA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Pipeline for Modelling and Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "H_7BXNYMULrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline Optmization\n",
        "* Feature Scaling\n",
        "* Feature Selection\n",
        "* Model"
      ],
      "metadata": {
        "id": "2t4w68uILe1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def PipelineClfSMOTE(model):\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "       (\"scaler\",StandardScaler() ),\n",
        "       (\"feat_selection\",SelectFromModel(model) ),\n",
        "       (\"model\",model ),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base"
      ],
      "outputs": [],
      "metadata": {
        "id": "PYR4hz6-Ldvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Class for hyperparameter Optimization and search best model"
      ],
      "metadata": {
        "id": "KM_hrtfjLj85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model=  PipelineClfSMOTE(self.models[key])\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X,y)\n",
        "            self.grid_searches[key] = gs    \n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                 'estimator': key,\n",
        "                 'min_score': min(scores),\n",
        "                 'max_score': max(scores),\n",
        "                 'mean_score': np.mean(scores),\n",
        "                 'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params,**d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]        \n",
        "                scores.append(r.reshape(len(params),1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params,all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "NpTcVDtQ5RMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Train and Test Set"
      ],
      "metadata": {
        "id": "eUcOp83jy0QG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Quick recap in our dataset"
      ],
      "metadata": {
        "id": "kQTZ_fRGyQFB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(df.shape)\n",
        "df.head(3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "wKjia1_VyQrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Split Train and Test Sets"
      ],
      "metadata": {
        "id": "KY3JCy2hq2OH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df.drop(['Churn'],axis=1),\n",
        "                                    df['Churn'],\n",
        "                                    test_size = 0.2,\n",
        "                                    random_state = 0,\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0vqzNI2zF1sZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Target Imbalance"
      ],
      "metadata": {
        "id": "4zBysp0tyqR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit DataCleaning And FeatureEngineering Pipeline\n",
        "  * It is used to process train data, so SMOTE can be applied before training the model"
      ],
      "metadata": {
        "id": "TfzOprjgq4bO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "MsQRvnn1GI_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how it looks like"
      ],
      "metadata": {
        "id": "Y4itUTGWFil1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train.head(3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "SJQNjPS5FfhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Train Set Target distribution"
      ],
      "metadata": {
        "id": "wuq3902arZAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()\n",
        "print(\"\\n* Class proportion on Train Set\\n\", y_train.value_counts(normalize=True).to_frame().round(2))\n",
        "print(\"\\n* Class proportion on Test Set\\n\", y_test.value_counts(normalize=True).to_frame().round(2))"
      ],
      "outputs": [],
      "metadata": {
        "id": "I28ACrp-rPgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SMOTE to balance Train Set target"
      ],
      "metadata": {
        "id": "-OgoR6lTrKqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "tP1JIwXNEsXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Train Set Target distribution after SMOTE"
      ],
      "metadata": {
        "id": "vTJO6V5zrdnw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()\n",
        "print(\"\\n* Class proportion on Train Set\\n\", y_train.value_counts(normalize=True).to_frame().round(2))\n",
        "print(\"\\n* Class proportion on Test Set\\n\",y_test.value_counts(normalize=True).to_frame().round(2))"
      ],
      "outputs": [],
      "metadata": {
        "id": "iQdvEvNRG80Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search CV - Sklearn"
      ],
      "metadata": {
        "id": "j2xTTXMayvo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use standard hyper parameters to find most suitable model"
      ],
      "metadata": {
        "id": "fizLJ_YQ6elb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define models and parameters, for Quick Search"
      ],
      "metadata": {
        "id": "8cwUldIJrjni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "models_quick_search = {\n",
        "    \"XGBClassifier\":XGBClassifier(random_state=0),\n",
        "    \"DecisionTreeClassifier\":DecisionTreeClassifier(random_state=0),\n",
        "    \"RandomForestClassifier\":RandomForestClassifier(random_state=0),\n",
        "    \"GradientBoostingClassifier\":GradientBoostingClassifier(random_state=0),\n",
        "    \"ExtraTreesClassifier\":ExtraTreesClassifier(random_state=0),\n",
        "    \"AdaBoostClassifier\":AdaBoostClassifier(random_state=0),\n",
        "    \"XGBClassifier\":XGBClassifier(random_state=0),\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=0),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"XGBClassifier\":{},\n",
        "    \"DecisionTreeClassifier\":{},\n",
        "    \"RandomForestClassifier\":{},\n",
        "    \"GradientBoostingClassifier\":{},\n",
        "    \"ExtraTreesClassifier\":{},\n",
        "    \"AdaBoostClassifier\":{},\n",
        "    \"XGBClassifier\":{},\n",
        "    \"LogisticRegression\":{},\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "kMgswohfKBda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick GridSearch CV - Binary Classifier"
      ],
      "metadata": {
        "id": "GXu0Ryeown7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import make_scorer, recall_score\n",
        "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ],
      "outputs": [],
      "metadata": {
        "id": "O7eLJcKEKBlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check results"
      ],
      "metadata": {
        "id": "g0bkL-IxwnJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ],
      "outputs": [],
      "metadata": {
        "id": "YpFOc7OAKMuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the best model"
      ],
      "metadata": {
        "id": "MfoII4RjwkF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ],
      "outputs": [],
      "metadata": {
        "id": "LvYM0pfNMHv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do extensive search on most suitable model to find best hyperparameter configuration"
      ],
      "metadata": {
        "id": "ewezVDt46jTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model and parameters, for Extensive Search"
      ],
      "metadata": {
        "id": "Z1WozH5frBQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "models_search = {\n",
        "    \"XGBClassifier\":XGBClassifier(random_state=0),\n",
        "}\n",
        "\n",
        "# documentation to help on hyperparameter list: \n",
        "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "\n",
        "# We will not conduct an extensive search, since the focus\n",
        "# is on how to combine all knowledge in an applied project.\n",
        "# In a workplace project, you may spend more time in this step\n",
        "params_search = {\n",
        "    \"XGBClassifier\":{\n",
        "        'model__learning_rate': [1e-1,1e-2,1e-3], \n",
        "        'model__max_depth': [3,10,None],\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "sDT_WMUErBRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extensive GridSearch CV - Binary Classifier"
      ],
      "metadata": {
        "id": "BP2Ua0FGrBRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import recall_score, make_scorer\n",
        "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WK1s893orBRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check results"
      ],
      "metadata": {
        "id": "l8oVKtHyr-X8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ],
      "outputs": [],
      "metadata": {
        "id": "8AFyZ6-pr9tN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the best model"
      ],
      "metadata": {
        "id": "olUcfuqYr9tO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ],
      "outputs": [],
      "metadata": {
        "id": "9_Bv3H9Cr9tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters for best model\n",
        "* We are saving this content for later"
      ],
      "metadata": {
        "id": "htAXEVFpwiBV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_parameters = grid_search_pipelines[best_model].best_params_\n",
        "best_parameters"
      ],
      "outputs": [],
      "metadata": {
        "id": "oDIt27RdKOG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the best clf pipeline"
      ],
      "metadata": {
        "id": "eAnJQlDlw1FE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ],
      "outputs": [],
      "metadata": {
        "id": "zLotNfy4MKDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assess feature importance"
      ],
      "metadata": {
        "id": "UgdxKijH6qJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* With the current model, we can assess with `.features_importances_`"
      ],
      "metadata": {
        "id": "n30pl2dowzW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_features = X_train.columns[pipeline_clf['feat_selection'].get_support()].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': X_train.columns[pipeline_clf['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        ".sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "best_features = df_feature_importance['Feature'].to_list() # re-assign best_features order\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "4XGczhv2uo2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will save the most important features to fit a new pipeline"
      ],
      "metadata": {
        "id": "K9QjeiVyCC6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_features_with_all_variables = best_features\n",
        "best_features_with_all_variables"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZRdCBsMrCJqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ],
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def confusion_matrix_and_report(X,y,pipeline,label_map):\n",
        "\n",
        "  prediction = pipeline.predict(X)\n",
        "\n",
        "  print('---  Confusion Matrix  ---')\n",
        "  print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "        columns=[ [\"Actual \" + sub for sub in label_map] ], \n",
        "        index= [ [\"Prediction \" + sub for sub in label_map ]]\n",
        "        ))\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "  print('---  Classification Report  ---')\n",
        "  print(classification_report(y, prediction),\"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train,y_train,X_test,y_test,pipeline,label_map):\n",
        "  print(\"#### Train Set #### \\n\")\n",
        "  confusion_matrix_and_report(X_train,y_train,pipeline,label_map)\n",
        "\n",
        "  print(\"#### Test Set ####\\n\")\n",
        "  confusion_matrix_and_report(X_test,y_test,pipeline,label_map)"
      ],
      "outputs": [],
      "metadata": {
        "id": "myG6tDSGan4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        " * We cross check with metrics defined at ML business case"
      ],
      "metadata": {
        "id": "qpUfEAGlW5aK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Churn', 'Churn'] \n",
        "                )"
      ],
      "outputs": [],
      "metadata": {
        "id": "umWjIvGMNLig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refit pipeline with best features"
      ],
      "metadata": {
        "id": "7WgttWjtHHOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New ML Pipeline"
      ],
      "metadata": {
        "id": "kCyOyebVHVmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In theory, a pipeline fitted **using only the most important features** has to give the same result as the one fitted with **all variables and feature selection**\n",
        "\n",
        "* However in this project we have a step for feature augmentation, which is to balance the target Train Set using SMOTE()\n",
        "* We should remember that the Train Set with all features is different from the Train Set with the best features we found (since it has less variables)\n",
        "* Therefore the Train Set after applying the SMOE() will be slightly different, which means the performance will be slightly different. We should expect that. What we can't expect is to have a big difference "
      ],
      "metadata": {
        "id": "R4PpI2sKC5IL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This new pipeline should consider only the set of most important features"
      ],
      "metadata": {
        "id": "jJtjiIaOCQgK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_features_with_all_variables"
      ],
      "outputs": [],
      "metadata": {
        "id": "5-QpXI-0CPpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rewrite ML pipeline for Data Cleaning and Feature Engineering"
      ],
      "metadata": {
        "id": "Km_-hW0f68DP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New Pipeline for DataCleaning And FeatureEngineering"
      ],
      "metadata": {
        "id": "EBeckIjkCa4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "\n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  variables = [ 'InternetService', 'Contract']\n",
        "                                                  )\n",
        "      ),\n",
        "       \n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base"
      ],
      "outputs": [],
      "metadata": {
        "id": "bc8ptvFiHJmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rewrite ML Pipeline for Modelling"
      ],
      "metadata": {
        "id": "uGNs9PU16_Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for Pipeline optmization"
      ],
      "metadata": {
        "id": "gpjmxzTbCXlg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Pipeline Optmization: Feature Scaling, and Model\n",
        "# there is no feature selection\n",
        "def PipelineClfSMOTE(model):\n",
        "  pipeline_base = Pipeline(\n",
        "      [\n",
        "       (\"scaler\",StandardScaler()),\n",
        "       # no feature selection here!!!\n",
        "       (\"model\",model ),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return pipeline_base\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "8E76QmoMEWA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Train Test Set, considering only with best features"
      ],
      "metadata": {
        "id": "75hfh3o5GhoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Split Train and Test Sets"
      ],
      "metadata": {
        "id": "2-yoBrBPGhoc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df.drop(['Churn'],axis=1),\n",
        "                                    df['Churn'],\n",
        "                                    test_size = 0.2,\n",
        "                                    random_state = 0,\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "x6dX0VeKGhod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We filter only the most important variables"
      ],
      "metadata": {
        "id": "c19a3t6jI6H6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train = X_train.filter(best_features_with_all_variables)\n",
        "X_test = X_test.filter(best_features_with_all_variables)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "p5Acb9T_GXjU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train.head(3)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_4Iz8QgtGXje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Target Imbalance"
      ],
      "metadata": {
        "id": "sjOcRGheGhof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fit DataCleaning And FeatureEngineering Pipeline\n",
        "  * It is used to process train data, so SMOTE can be applied before training the model"
      ],
      "metadata": {
        "id": "277qb3CgGhof"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KbQda_pcGhof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check Train Set Target distribution"
      ],
      "metadata": {
        "id": "EQxIFw3KGhog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()\n",
        "print(\"\\n* Class proportion on Train Set\\n\", y_train.value_counts(normalize=True).to_frame().round(2))\n",
        "print(\"\\n* Class proportion on Test Set\\n\",y_test.value_counts(normalize=True).to_frame().round(2))"
      ],
      "outputs": [],
      "metadata": {
        "id": "7ZQyth-2Ghog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use SMOTE to balance Train Set target"
      ],
      "metadata": {
        "id": "N9FbCbIrGhoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OtbWft5VGhoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check Train Set Target distribution after SMOTE"
      ],
      "metadata": {
        "id": "YozwzI9eGhoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()\n",
        "print(\"\\n* Class proportion on Train Set\\n\", y_train.value_counts(normalize=True).to_frame().round(2))\n",
        "print(\"\\n* Class proportion on Test Set\\n\",y_test.value_counts(normalize=True).to_frame().round(2))"
      ],
      "outputs": [],
      "metadata": {
        "id": "TyL99cYMGhoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search CV: Sklearn"
      ],
      "metadata": {
        "id": "b_WjvD_QIJ_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using most suitable model from last section and it best hyper parameter configuration"
      ],
      "metadata": {
        "id": "ESkqrySI7N6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the same model fomr the last GridCV search"
      ],
      "metadata": {
        "id": "F6fFaXDOIJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "models_search"
      ],
      "outputs": [],
      "metadata": {
        "id": "H7F0z__h1qSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the best parameters from the last GridCV search "
      ],
      "metadata": {
        "id": "qRteBPgd3ldU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_parameters"
      ],
      "outputs": [],
      "metadata": {
        "id": "IbGNBeZk3V8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to type in manually, since the hyperparameter values has to be a list. The previous dictonary is not in this format"
      ],
      "metadata": {
        "id": "YlLJP5Ds3rYp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "params_search = {'XGBClassifier':  {\n",
        "    'model__learning_rate': [0.01],   # the value should be in []\n",
        "    'model__max_depth': [3]}, # the value should be in []\n",
        "}\n",
        "params_search"
      ],
      "outputs": [],
      "metadata": {
        "id": "9bC8RmE-2Mi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch CV"
      ],
      "metadata": {
        "id": "GrRJNywsIJ_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.metrics import recall_score, make_scorer\n",
        "quick_search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "quick_search.fit(X_train, y_train,\n",
        "                 scoring =  make_scorer(recall_score, pos_label=1),\n",
        "                 n_jobs=-1, cv=5)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yv5nO6cJP9fX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check results"
      ],
      "metadata": {
        "id": "Yr_Yu9ykIJ_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ],
      "outputs": [],
      "metadata": {
        "id": "fqIk1g95IJ_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the best model"
      ],
      "metadata": {
        "id": "_inGrVOBIJ_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ],
      "outputs": [],
      "metadata": {
        "id": "RvqOaKpDIJ_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters for best model"
      ],
      "metadata": {
        "id": "I6CK0D0nIJ_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "grid_search_pipelines[best_model].best_params_"
      ],
      "outputs": [],
      "metadata": {
        "id": "8NtLgX1aIJ_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the best clf pipeline"
      ],
      "metadata": {
        "id": "tZcP3yXpIJ_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ],
      "outputs": [],
      "metadata": {
        "id": "P7Qe2jFEIJ_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assess feature importance"
      ],
      "metadata": {
        "id": "qXEXWvsb7Su0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "best_features = X_train.columns\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        ".sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "T8UGZ5bnIJ_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ],
      "metadata": {
        "id": "nQF20xan7VuK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Churn', 'Churn'] \n",
        "                )"
      ],
      "outputs": [],
      "metadata": {
        "id": "1cpCj2lLHxB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push files to Repo"
      ],
      "metadata": {
        "id": "oBVunRgBqIXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will generate the following files\n",
        "* Train set\n",
        "* Test set\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* features importance plot"
      ],
      "metadata": {
        "id": "yxnlKI5SJcoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/predict_churn/{version}'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "outputs": [],
      "metadata": {
        "id": "16bIOgs3J7OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pay attention that the Train set and Test set are not in the same format as when it was splitted\n",
        "* That is due to the fact we needed to apply SMOTE to the Train Set"
      ],
      "metadata": {
        "id": "MaVV7t2SUjcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Set"
      ],
      "metadata": {
        "id": "3e-gC6sa7hpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* note that the variables **are transformed already** in X_train and the shape is 8266 - after SMOTE was appllied"
      ],
      "metadata": {
        "id": "hHZUZKJ5JiKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X_train.shape)\n",
        "X_train.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Sc4fzrdTJno1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Qzq7DgVTJnv3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_train"
      ],
      "outputs": [],
      "metadata": {
        "id": "DzPsdNGX9gtf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "FMoT1cJ39g26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set"
      ],
      "metadata": {
        "id": "OYatlgsj7pbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* note that the variables are transformed already in X_test"
      ],
      "metadata": {
        "id": "tEKp3-dJJn3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(X_test.shape)\n",
        "X_test.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "9UMg2vPtJqxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "uz2OqPW6Jqzv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_test"
      ],
      "outputs": [],
      "metadata": {
        "id": "4pPTVz219xj3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ap7fYYAm9xsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Pipelines: Data Cleaning and Feat Eng pipeline and Modelling Pipeline"
      ],
      "metadata": {
        "id": "_ufHAplN7tdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will save 2 pipelines: \n",
        "* Both should be used in conjuntion to predict Live Data\n",
        "* To predict on Train Set, Test Set we use only pipeline_clf, since the data is already processed\n",
        "\n",
        "\n",
        "\n",
        "Pipeline responsible for Data Cleaning and Feature Engineering\n"
      ],
      "metadata": {
        "id": "XAbbAO2r248W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline_data_cleaning_feat_eng"
      ],
      "outputs": [],
      "metadata": {
        "id": "XCcAlvoG3CRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "joblib.dump(value=pipeline_data_cleaning_feat_eng ,\n",
        "            filename=f\"{file_path}/clf_pipeline_data_cleaning_feat_eng.pkl\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "AaHdCf4HKBLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pipeline responsible for Feature Scaling, and Model"
      ],
      "metadata": {
        "id": "XE-iU6TL3LVI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pipeline_clf"
      ],
      "outputs": [],
      "metadata": {
        "id": "_zEBxfvBqI29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "joblib.dump(value=pipeline_clf ,\n",
        "            filename=f\"{file_path}/clf_pipeline_model.pkl\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ObL5Iz8tKdsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance plot"
      ],
      "metadata": {
        "id": "yqEUyLG27v9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "wBiqB55L1Qhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
      ],
      "outputs": [],
      "metadata": {
        "id": "NR0taWpn1RuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "3wUzYrfu71Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Push** generated/new files from this Session to GitHub repo"
      ],
      "metadata": {
        "id": "UGutiDskJM8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Git status"
      ],
      "metadata": {
        "id": "FJ7D-6nrJM8S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "! git status"
      ],
      "outputs": [],
      "metadata": {
        "id": "u0Y1wbBXJM8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Git commit"
      ],
      "metadata": {
        "id": "CrVumSunJM8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "CommitMsg = \"added-files-predict-churn\"\n",
        "! git add .\n",
        "! git commit -m {CommitMsg}"
      ],
      "outputs": [],
      "metadata": {
        "id": "zZXJKvjIJM8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Git Push"
      ],
      "metadata": {
        "id": "_QlQaNfdJM8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "! git push origin main"
      ],
      "outputs": [],
      "metadata": {
        "id": "WsEe0tClJM8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well done! Clear the outputs before you move on to the next notebook!"
      ],
      "metadata": {}
    }
  ]
}